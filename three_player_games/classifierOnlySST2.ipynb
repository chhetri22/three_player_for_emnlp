{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15403 words has been switched.\n"
     ]
    }
   ],
   "source": [
    "def generate_data(batch):\n",
    "    # sort for rnn happiness\n",
    "    batch.sort_values(\"counts\", inplace=True, ascending=False)\n",
    "    \n",
    "    x_mask = np.stack(batch[\"mask\"], axis=0)\n",
    "    # drop all zero columns\n",
    "    zero_col_idxs = np.argwhere(np.all(x_mask[...,:] == 0, axis=0))\n",
    "    x_mask = np.delete(x_mask, zero_col_idxs, axis=1)\n",
    "\n",
    "    x_mat = np.stack(batch[\"tokens\"], axis=0)\n",
    "    # drop all zero columns\n",
    "    x_mat = np.delete(x_mat, zero_col_idxs, axis=1)\n",
    "\n",
    "    y_vec = np.stack(batch[\"label\"], axis=0)\n",
    "    \n",
    "    batch_x_ = Variable(torch.from_numpy(x_mat)).to(torch.int64)\n",
    "    batch_m_ = Variable(torch.from_numpy(x_mask)).type(torch.FloatTensor)\n",
    "    batch_y_ = Variable(torch.from_numpy(y_vec)).to(torch.int64)\n",
    "\n",
    "    if args.cuda:\n",
    "        batch_x_ = batch_x_.cuda()\n",
    "        batch_m_ = batch_m_.cuda()\n",
    "        batch_y_ = batch_y_.cuda()\n",
    "\n",
    "    return batch_x_, batch_m_, batch_y_\n",
    "    \n",
    "glove_path = os.path.join(\"..\", \"datasets\", \"glove.6B.100d.txt\")\n",
    "COUNT_THRESH = 3\n",
    "DATA_FOLDER = os.path.join(\"../../sentiment_dataset/data/\")\n",
    "LABEL_COL = \"label\"\n",
    "TEXT_COL = \"sentence\"\n",
    "TOKEN_CUTOFF = 70\n",
    "\n",
    "def generate_tokens_glove(word_vocab, text):\n",
    "    indexed_text = [word_vocab[word] if (counts[word] > COUNT_THRESH) else word_vocab[\"<UNK>\"] for word in text.split()]\n",
    "    pad_length = TOKEN_CUTOFF - len(indexed_text)\n",
    "    mask = [1] * len(indexed_text) + [0] * pad_length\n",
    "\n",
    "    indexed_text = indexed_text + [word_vocab[\"<PAD>\"]] * pad_length\n",
    "\n",
    "    return np.array(indexed_text), np.array(mask)\n",
    "\n",
    "def get_all_tokens_glove(data):\n",
    "    l = []\n",
    "    m = []\n",
    "    counts = []\n",
    "    for sentence in data:\n",
    "        token_list, mask = generate_tokens_glove(word_vocab, sentence)\n",
    "        l.append(token_list)\n",
    "        m.append(mask)\n",
    "        counts.append(np.sum(mask))\n",
    "    tokens = pd.DataFrame({\"tokens\": l, \"mask\": m, \"counts\": counts})\n",
    "    return tokens\n",
    "\n",
    "def build_vocab(df):\n",
    "    d = {\"<PAD>\":0, \"<UNK>\":1}\n",
    "    counts = {}\n",
    "    for i in range(len(df)):\n",
    "        sentence = df.iloc[i][TEXT_COL]\n",
    "        for word in sentence.split():\n",
    "            if word not in d:\n",
    "                d[word] = len(d)\n",
    "                counts[word] = 1\n",
    "            else:\n",
    "                counts[word] += 1\n",
    "    reverse_d = {v: k for k, v in d.items()}\n",
    "    return d, reverse_d, counts\n",
    "\n",
    "def initial_embedding(word_vocab, embedding_size, embedding_path=None): \n",
    "    vocab_size = len(word_vocab)\n",
    "    # initialize a numpy embedding matrix \n",
    "\n",
    "    embeddings = 0.1*np.random.randn(vocab_size, embedding_size).astype(np.float32)\n",
    "\n",
    "    # replace the <PAD> embedding by all zero\n",
    "    embeddings[0, :] = np.zeros(embedding_size, dtype=np.float32)\n",
    "\n",
    "    if embedding_path and os.path.isfile(embedding_path):\n",
    "        f = open(embedding_path, \"r\", encoding=\"utf8\")\n",
    "        counter = 0\n",
    "        for line in f:\n",
    "            data = line.strip().split(\" \")\n",
    "            word = data[0].strip()\n",
    "            embedding = data[1::]\n",
    "            embedding = list(map(np.float32, embedding))\n",
    "            if word in word_vocab:\n",
    "                embeddings[word_vocab[word], :] = embedding\n",
    "                counter += 1\n",
    "        f.close()\n",
    "        print(\"%d words has been switched.\"%counter)\n",
    "    else:\n",
    "        print(\"embedding is initialized fully randomly.\")\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def load_data(fpath):\n",
    "    df_dict = {LABEL_COL: [], TEXT_COL: []}\n",
    "    with open(fpath, 'r') as f:\n",
    "        label_start = 0\n",
    "        sentence_start = 2\n",
    "        for line in f:\n",
    "            label = int(line[label_start])\n",
    "            sentence = line[sentence_start:]\n",
    "            df_dict[LABEL_COL].append(label)\n",
    "            df_dict[TEXT_COL].append(sentence)\n",
    "    return pd.DataFrame.from_dict(df_dict)\n",
    "\n",
    "\n",
    "df_train = load_data(os.path.join(DATA_FOLDER, 'stsa.binary.train'))\n",
    "df_test = load_data(os.path.join(DATA_FOLDER, 'stsa.binary.test'))\n",
    "# TODO combine train and test dataset into df_all\n",
    "df_all = pd.concat([df_train, df_test])\n",
    "\n",
    "word_vocab, reverse_word_vocab, counts = build_vocab(df_all)\n",
    "embeddings = initial_embedding(word_vocab, 100, glove_path)\n",
    "\n",
    "# create training and testing labels\n",
    "y_train = df_train[LABEL_COL]\n",
    "y_test = df_test[LABEL_COL]\n",
    "\n",
    "# create training and testing inputs\n",
    "X_train = df_train[TEXT_COL]\n",
    "X_test = df_test[TEXT_COL]\n",
    "\n",
    "df_train = pd.concat([df_train, get_all_tokens_glove(X_train)], axis=1)\n",
    "df_test = pd.concat([df_test, get_all_tokens_glove(X_test)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Argument():\n",
    "    def __init__(self):\n",
    "        self.cell_type = 'GRU'\n",
    "        self.embedding_dim = 100\n",
    "        self.num_labels = 2\n",
    "        self.hidden_dim = 400\n",
    "        self.layer_num = 1\n",
    "        self.cuda = True\n",
    "\n",
    "# classes needed for Rationale3Player\n",
    "class RnnModel(nn.Module):\n",
    "    def __init__(self, args, input_dim):\n",
    "        \"\"\"\n",
    "        args.hidden_dim -- dimension of filters\n",
    "        args.embedding_dim -- dimension of word embeddings\n",
    "        args.layer_num -- number of RNN layers   \n",
    "        args.cell_type -- type of RNN cells, GRU or LSTM\n",
    "        \"\"\"\n",
    "        super(RnnModel, self).__init__()\n",
    "        \n",
    "        self.args = args\n",
    " \n",
    "        if args.cell_type == 'GRU':\n",
    "            self.rnn_layer = nn.GRU(input_size=input_dim, \n",
    "                                    hidden_size=args.hidden_dim//2, \n",
    "                                    num_layers=args.layer_num, bidirectional=True)\n",
    "        elif args.cell_type == 'LSTM':\n",
    "            self.rnn_layer = nn.LSTM(input_size=input_dim, \n",
    "                                     hidden_size=args.hidden_dim//2, \n",
    "                                     num_layers=args.layer_num, bidirectional=True)\n",
    "    \n",
    "    def forward(self, embeddings, mask=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embeddings -- sequence of word embeddings, (batch_size, sequence_length, embedding_dim)\n",
    "            mask -- a float tensor of masks, (batch_size, length)\n",
    "        Outputs:\n",
    "            hiddens -- sentence embedding tensor, (batch_size, hidden_dim, sequence_length)\n",
    "        \"\"\"\n",
    "        embeddings_ = embeddings.transpose(0, 1) #(sequence_length, batch_size, embedding_dim)\n",
    "        \n",
    "        if mask is not None:\n",
    "            seq_lengths = list(torch.sum(mask, dim=1).cpu().data.numpy())\n",
    "            seq_lengths = list(map(int, seq_lengths))\n",
    "            inputs_ = torch.nn.utils.rnn.pack_padded_sequence(embeddings_, seq_lengths)\n",
    "        else:\n",
    "            inputs_ = embeddings_\n",
    "        \n",
    "        hidden, _ = self.rnn_layer(inputs_) #(sequence_length, batch_size, hidden_dim (* 2 if bidirectional))\n",
    "        \n",
    "        if mask is not None:\n",
    "            hidden, _ = torch.nn.utils.rnn.pad_packed_sequence(hidden) #(length, batch_size, hidden_dim)\n",
    "        \n",
    "        return hidden.permute(1, 2, 0) #(batch_size, hidden_dim, sequence_length)\n",
    "\n",
    "class ClassifierModule(nn.Module):\n",
    "    '''\n",
    "    classifier for both E and E_anti models provided with RNP paper code\n",
    "    '''\n",
    "    def __init__(self, embeddings, args):\n",
    "        super(ClassifierModule, self).__init__()\n",
    "        self.args = args\n",
    "        self.num_labels = args.num_labels\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "        self.mlp_hidden_dim = args.mlp_hidden_dim #50\n",
    "        self.input_dim = args.embedding_dim\n",
    "        \n",
    "        self.encoder = RnnModel(self.args, self.input_dim)\n",
    "        self.predictor = nn.Linear(self.hidden_dim, self.num_labels)\n",
    "        \n",
    "        self.NEG_INF = -1.0e6\n",
    "\n",
    "        self.vocab_size, self.embedding_dim = embeddings.shape\n",
    "        self.embed_layer = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        self.embed_layer.weight.data = torch.from_numpy(embeddings)\n",
    "        self.embed_layer.weight.requires_grad = True #TODO try false? \n",
    "\n",
    "    def forward(self, x, z, mask):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            word_embeddings -- torch Variable in shape of (batch_size, length, embed_dim)\n",
    "            z -- rationale (batch_size, length)\n",
    "            mask -- torch Variable in shape of (batch_size, length)\n",
    "        Outputs:\n",
    "            predict -- (batch_size, num_label)\n",
    "        \"\"\"        \n",
    "        word_embeddings = self.embed_layer(x) #(batch_size, length, embedding_dim)\n",
    "\n",
    "        masked_input = word_embeddings * z.unsqueeze(-1)\n",
    "        hiddens = self.encoder(masked_input, mask)\n",
    "        \n",
    "        max_hidden = torch.max(hiddens + (1 - mask * z).unsqueeze(1) * self.NEG_INF, dim=2)[0]\n",
    "        \n",
    "        predict = self.predictor(max_hidden)\n",
    "        return predict\n",
    "    \n",
    "    def test(self, df_test):\n",
    "        self.eval()\n",
    "        batch_size = 1000\n",
    "        batch = df_test.sample(batch_size, replace=True)\n",
    "        batch_x_, batch_m_, batch_y_ = generate_data(batch)\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        z = torch.ones_like(batch_x_).type(torch.cuda.FloatTensor)\n",
    "        _, predict =  torch.max(self.forward(batch_x_, z, batch_m_), dim=1)\n",
    "        accuracy = (predict == batch_y_).sum().item() / batch_size\n",
    "\n",
    "        return accuracy\n",
    "    \n",
    "    def fit(self, df_train, df_test):\n",
    "        train_acc = []\n",
    "        test_acc = []\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(self.parameters(), lr=0.001, momentum=0.9) #experiment with learning rate\n",
    "        \n",
    "        for i in range(5000):\n",
    "            self.train()\n",
    "            batch = df_train.sample(40, replace=True)\n",
    "            batch_x_, batch_m_, batch_y_ = generate_data(batch)\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "\n",
    "            z = torch.ones_like(batch_x_).type(torch.cuda.FloatTensor)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = self.forward(batch_x_, z, batch_m_)\n",
    "            loss = criterion(outputs, batch_y_)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            if i % 100 == 0:\n",
    "                print(\"\")\n",
    "                print(\"Iteration \", i)\n",
    "                test_accuracy = self.test(df_test)\n",
    "                train_accuracy = self.test(df_train)\n",
    "                \n",
    "                print(\"Test accuracy: \", test_accuracy)\n",
    "                print(\"Train accuracy: \", train_accuracy)\n",
    "                test_acc.append(test_accuracy)\n",
    "                train_acc.append(train_accuracy)\n",
    "\n",
    "        print('Finished Training')\n",
    "        return test_acc, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration  0\n",
      "Accuracy:  0.504\n",
      "\n",
      "Iteration  100\n",
      "Accuracy:  0.511\n",
      "\n",
      "Iteration  200\n",
      "Accuracy:  0.61\n",
      "\n",
      "Iteration  300\n",
      "Accuracy:  0.604\n",
      "\n",
      "Iteration  400\n",
      "Accuracy:  0.582\n",
      "\n",
      "Iteration  500\n",
      "Accuracy:  0.634\n",
      "\n",
      "Iteration  600\n",
      "Accuracy:  0.649\n",
      "\n",
      "Iteration  700\n",
      "Accuracy:  0.619\n",
      "\n",
      "Iteration  800\n",
      "Accuracy:  0.695\n",
      "\n",
      "Iteration  900\n",
      "Accuracy:  0.726\n",
      "\n",
      "Iteration  1000\n",
      "Accuracy:  0.604\n",
      "\n",
      "Iteration  1100\n",
      "Accuracy:  0.706\n",
      "\n",
      "Iteration  1200\n",
      "Accuracy:  0.702\n",
      "\n",
      "Iteration  1300\n",
      "Accuracy:  0.715\n",
      "\n",
      "Iteration  1400\n",
      "Accuracy:  0.699\n",
      "\n",
      "Iteration  1500\n",
      "Accuracy:  0.741\n",
      "\n",
      "Iteration  1600\n",
      "Accuracy:  0.738\n",
      "\n",
      "Iteration  1700\n",
      "Accuracy:  0.755\n",
      "\n",
      "Iteration  1800\n",
      "Accuracy:  0.767\n",
      "\n",
      "Iteration  1900\n",
      "Accuracy:  0.762\n",
      "\n",
      "Iteration  2000\n",
      "Accuracy:  0.775\n",
      "\n",
      "Iteration  2100\n",
      "Accuracy:  0.784\n",
      "\n",
      "Iteration  2200\n",
      "Accuracy:  0.774\n",
      "\n",
      "Iteration  2300\n",
      "Accuracy:  0.769\n",
      "\n",
      "Iteration  2400\n",
      "Accuracy:  0.774\n",
      "\n",
      "Iteration  2500\n",
      "Accuracy:  0.809\n",
      "\n",
      "Iteration  2600\n",
      "Accuracy:  0.779\n",
      "\n",
      "Iteration  2700\n",
      "Accuracy:  0.783\n",
      "\n",
      "Iteration  2800\n",
      "Accuracy:  0.781\n",
      "\n",
      "Iteration  2900\n",
      "Accuracy:  0.809\n",
      "\n",
      "Iteration  3000\n",
      "Accuracy:  0.783\n",
      "\n",
      "Iteration  3100\n",
      "Accuracy:  0.774\n",
      "\n",
      "Iteration  3200\n",
      "Accuracy:  0.792\n",
      "\n",
      "Iteration  3300\n",
      "Accuracy:  0.791\n",
      "\n",
      "Iteration  3400\n",
      "Accuracy:  0.784\n",
      "\n",
      "Iteration  3500\n",
      "Accuracy:  0.795\n",
      "\n",
      "Iteration  3600\n",
      "Accuracy:  0.779\n",
      "\n",
      "Iteration  3700\n",
      "Accuracy:  0.779\n",
      "\n",
      "Iteration  3800\n",
      "Accuracy:  0.797\n",
      "\n",
      "Iteration  3900\n",
      "Accuracy:  0.802\n",
      "\n",
      "Iteration  4000\n",
      "Accuracy:  0.809\n",
      "\n",
      "Iteration  4100\n",
      "Accuracy:  0.773\n",
      "\n",
      "Iteration  4200\n",
      "Accuracy:  0.779\n",
      "\n",
      "Iteration  4300\n",
      "Accuracy:  0.78\n",
      "\n",
      "Iteration  4400\n",
      "Accuracy:  0.799\n",
      "\n",
      "Iteration  4500\n",
      "Accuracy:  0.799\n",
      "\n",
      "Iteration  4600\n",
      "Accuracy:  0.785\n",
      "\n",
      "Iteration  4700\n",
      "Accuracy:  0.817\n",
      "\n",
      "Iteration  4800\n",
      "Accuracy:  0.791\n",
      "\n",
      "Iteration  4900\n",
      "Accuracy:  0.82\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "args = Argument()\n",
    "cls = ClassifierModule(embeddings, args)\n",
    "cls.cuda()\n",
    "test_acc, train_acc = cls.fit(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
