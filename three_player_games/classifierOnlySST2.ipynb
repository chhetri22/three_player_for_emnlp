{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(fpath):\n",
    "    df_dict = {LABEL_COL: [], TEXT_COL: []}\n",
    "    with open(fpath, 'r') as f:\n",
    "        label_start = 0\n",
    "        sentence_start = 2\n",
    "        for line in f:\n",
    "            label = int(line[label_start])\n",
    "            sentence = line[sentence_start:]\n",
    "            df_dict[LABEL_COL].append(label)\n",
    "            df_dict[TEXT_COL].append(sentence)\n",
    "    return pd.DataFrame.from_dict(df_dict)\n",
    "\n",
    "def generate_data(batch):\n",
    "    # sort for rnn happiness\n",
    "    batch.sort_values(\"counts\", inplace=True, ascending=False)\n",
    "    \n",
    "    x_mask = np.stack(batch[\"mask\"], axis=0)\n",
    "    # drop all zero columns\n",
    "    zero_col_idxs = np.argwhere(np.all(x_mask[...,:] == 0, axis=0))\n",
    "    x_mask = np.delete(x_mask, zero_col_idxs, axis=1)\n",
    "\n",
    "    x_mat = np.stack(batch[\"tokens\"], axis=0)\n",
    "    # drop all zero columns\n",
    "    x_mat = np.delete(x_mat, zero_col_idxs, axis=1)\n",
    "\n",
    "    y_vec = np.stack(batch[\"label\"], axis=0)\n",
    "    \n",
    "    batch_x_ = Variable(torch.from_numpy(x_mat)).to(torch.int64)\n",
    "    batch_m_ = Variable(torch.from_numpy(x_mask)).type(torch.FloatTensor)\n",
    "    batch_y_ = Variable(torch.from_numpy(y_vec)).to(torch.int64)\n",
    "\n",
    "    if args.cuda:\n",
    "        batch_x_ = batch_x_.cuda()\n",
    "        batch_m_ = batch_m_.cuda()\n",
    "        batch_y_ = batch_y_.cuda()\n",
    "\n",
    "    return batch_x_, batch_m_, batch_y_\n",
    "\n",
    "glove_path = os.path.join(\"..\", \"datasets\", \"glove.6B.100d.txt\")\n",
    "COUNT_THRESH = 3\n",
    "DATA_FOLDER = os.path.join(\"../../sentiment_dataset/data/\")\n",
    "LABEL_COL = \"label\"\n",
    "TEXT_COL = \"sentence\"\n",
    "TOKEN_CUTOFF = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-7dd16f81aa31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[0mword_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse_word_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitial_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglove_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;31m# create training and testing labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-7dd16f81aa31>\u001b[0m in \u001b[0;36minitial_embedding\u001b[1;34m(word_vocab, embedding_size, embedding_path)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\rnp_env\\lib\\codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    317\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m         \u001b[1;31m# decode input (taking the buffer into account)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_tokens_glove(word_vocab, text):\n",
    "    indexed_text = [word_vocab[word] if (counts[word] > COUNT_THRESH) else word_vocab[\"<UNK>\"] for word in text.split()]\n",
    "    pad_length = TOKEN_CUTOFF - len(indexed_text)\n",
    "    mask = [1] * len(indexed_text) + [0] * pad_length\n",
    "\n",
    "    indexed_text = indexed_text + [word_vocab[\"<PAD>\"]] * pad_length\n",
    "\n",
    "    return np.array(indexed_text), np.array(mask)\n",
    "\n",
    "def get_all_tokens_glove(data):\n",
    "    l = []\n",
    "    m = []\n",
    "    counts = []\n",
    "    for sentence in data:\n",
    "        token_list, mask = generate_tokens_glove(word_vocab, sentence)\n",
    "        l.append(token_list)\n",
    "        m.append(mask)\n",
    "        counts.append(np.sum(mask))\n",
    "    tokens = pd.DataFrame({\"tokens\": l, \"mask\": m, \"counts\": counts})\n",
    "    return tokens\n",
    "\n",
    "def build_vocab(df):\n",
    "    d = {\"<PAD>\":0, \"<UNK>\":1}\n",
    "    counts = {}\n",
    "    for i in range(len(df)):\n",
    "        sentence = df.iloc[i][TEXT_COL]\n",
    "        for word in sentence.split():\n",
    "            if word not in d:\n",
    "                d[word] = len(d)\n",
    "                counts[word] = 1\n",
    "            else:\n",
    "                counts[word] += 1\n",
    "    reverse_d = {v: k for k, v in d.items()}\n",
    "    return d, reverse_d, counts\n",
    "\n",
    "def initial_embedding(word_vocab, embedding_size, embedding_path=None): \n",
    "    vocab_size = len(word_vocab)\n",
    "    # initialize a numpy embedding matrix \n",
    "\n",
    "    embeddings = 0.1*np.random.randn(vocab_size, embedding_size).astype(np.float32)\n",
    "\n",
    "    # replace the <PAD> embedding by all zero\n",
    "    embeddings[0, :] = np.zeros(embedding_size, dtype=np.float32)\n",
    "\n",
    "    if embedding_path and os.path.isfile(embedding_path):\n",
    "        f = open(embedding_path, \"r\", encoding=\"utf8\")\n",
    "        counter = 0\n",
    "        for line in f:\n",
    "            data = line.strip().split(\" \")\n",
    "            word = data[0].strip()\n",
    "            embedding = data[1::]\n",
    "            embedding = list(map(np.float32, embedding))\n",
    "            if word in word_vocab:\n",
    "                embeddings[word_vocab[word], :] = embedding\n",
    "                counter += 1\n",
    "        f.close()\n",
    "        print(\"%d words has been switched.\"%counter)\n",
    "    else:\n",
    "        print(\"embedding is initialized fully randomly.\")\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "df_train = load_data(os.path.join(DATA_FOLDER, 'stsa.binary.train'))\n",
    "df_test = load_data(os.path.join(DATA_FOLDER, 'stsa.binary.test'))\n",
    "# TODO combine train and test dataset into df_all\n",
    "df_all = pd.concat([df_train, df_test])\n",
    "\n",
    "word_vocab, reverse_word_vocab, counts = build_vocab(df_all)\n",
    "embeddings = initial_embedding(word_vocab, 100, glove_path)\n",
    "\n",
    "# create training and testing labels\n",
    "y_train = df_train[LABEL_COL]\n",
    "y_test = df_test[LABEL_COL]\n",
    "\n",
    "# create training and testing inputs\n",
    "X_train = df_train[TEXT_COL]\n",
    "X_test = df_test[TEXT_COL]\n",
    "\n",
    "df_train = pd.concat([df_train, get_all_tokens_glove(X_train)], axis=1)\n",
    "df_test = pd.concat([df_test, get_all_tokens_glove(X_test)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "def generate_tokens(tokenizer, text):\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    tokenized_text = tokenized_text[:TOKEN_CUTOFF - 2]\n",
    "    tokenized_text = [\"[CLS]\"] + tokenized_text + [\"[SEP]\"]\n",
    "    pad_length = TOKEN_CUTOFF - len(tokenized_text)\n",
    "    mask = [1] * len(tokenized_text) + [0] * pad_length\n",
    "    \n",
    "    tokenized_text = tokenized_text + [\"[PAD]\"] * pad_length\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    \n",
    "    return np.array(indexed_tokens), np.array(mask)\n",
    "    \n",
    "def get_all_tokens(data):\n",
    "    l = []\n",
    "    m = []\n",
    "    counts = []\n",
    "    for sentence in data:\n",
    "        token_list, mask = generate_tokens(tokenizer, sentence)\n",
    "        l.append(token_list)\n",
    "        m.append(mask)\n",
    "        counts.append(np.sum(mask))\n",
    "    tokens = pd.DataFrame({\"tokens\": l, \"mask\": m, \"counts\": counts})\n",
    "    return tokens\n",
    "\n",
    "df_train = load_data(os.path.join(DATA_FOLDER, 'stsa.binary.train'))\n",
    "df_test = load_data(os.path.join(DATA_FOLDER, 'stsa.binary.test'))\n",
    "# TODO combine train and test dataset into df_all\n",
    "df_all = pd.concat([df_train, df_test])\n",
    "\n",
    "# create training and testing labels\n",
    "y_train = df_train[LABEL_COL]\n",
    "y_test = df_test[LABEL_COL]\n",
    "\n",
    "# create training and testing inputs\n",
    "X_train = df_train[TEXT_COL]\n",
    "X_test = df_test[TEXT_COL]\n",
    "\n",
    "df_train = pd.concat([df_train, get_all_tokens(X_train)], axis=1)\n",
    "df_test = pd.concat([df_test, get_all_tokens(X_test)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_func(tokens):\n",
    "    ones_mask = torch.ones_like(tokens).type(torch.cuda.FloatTensor)\n",
    "    if args.cuda:\n",
    "        ones_mask = ones_mask.cuda()\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(tokens, ones_mask)[0]\n",
    "    return embeddings\n",
    "\n",
    "# classes needed for Rationale3Player\n",
    "class RnnModel(nn.Module):\n",
    "    def __init__(self, args, input_dim):\n",
    "        \"\"\"\n",
    "        args.hidden_dim -- dimension of filters\n",
    "        args.embedding_dim -- dimension of word embeddings\n",
    "        args.layer_num -- number of RNN layers   \n",
    "        args.cell_type -- type of RNN cells, GRU or LSTM\n",
    "        \"\"\"\n",
    "        super(RnnModel, self).__init__()\n",
    "        \n",
    "        self.args = args\n",
    " \n",
    "        if args.cell_type == 'GRU':\n",
    "            self.rnn_layer = nn.GRU(input_size=input_dim, \n",
    "                                    hidden_size=args.hidden_dim//2, \n",
    "                                    num_layers=args.layer_num, bidirectional=True)\n",
    "        elif args.cell_type == 'LSTM':\n",
    "            self.rnn_layer = nn.LSTM(input_size=input_dim, \n",
    "                                     hidden_size=args.hidden_dim//2, \n",
    "                                     num_layers=args.layer_num, bidirectional=True)\n",
    "    \n",
    "    def forward(self, embeddings, mask=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embeddings -- sequence of word embeddings, (batch_size, sequence_length, embedding_dim)\n",
    "            mask -- a float tensor of masks, (batch_size, length)\n",
    "        Outputs:\n",
    "            hiddens -- sentence embedding tensor, (batch_size, hidden_dim, sequence_length)\n",
    "        \"\"\"\n",
    "        embeddings_ = embeddings.transpose(0, 1) #(sequence_length, batch_size, embedding_dim)\n",
    "\n",
    "        if mask is not None:\n",
    "            seq_lengths = list(torch.sum(mask, dim=1).cpu().data.numpy())\n",
    "            seq_lengths = list(map(int, seq_lengths))\n",
    "            inputs_ = torch.nn.utils.rnn.pack_padded_sequence(embeddings_, seq_lengths)\n",
    "        else:\n",
    "            inputs_ = embeddings_\n",
    "        \n",
    "        hidden, _ = self.rnn_layer(inputs_) #(sequence_length, batch_size, hidden_dim (* 2 if bidirectional))\n",
    "        \n",
    "        if mask is not None:\n",
    "            hidden, _ = torch.nn.utils.rnn.pad_packed_sequence(hidden) #(length, batch_size, hidden_dim)\n",
    "        \n",
    "        return hidden.permute(1, 2, 0) #(batch_size, hidden_dim, sequence_length)\n",
    "\n",
    "class ClassifierModule(nn.Module):\n",
    "    '''\n",
    "    classifier for both E and E_anti models provided with RNP paper code\n",
    "    '''\n",
    "    def __init__(self, embeddings, args):\n",
    "        super(ClassifierModule, self).__init__()\n",
    "        self.args = args\n",
    "        self.num_labels = args.num_labels\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "        self.input_dim = args.embedding_dim\n",
    "        \n",
    "        self.encoder = RnnModel(self.args, self.input_dim)\n",
    "        self.predictor = nn.Linear(self.hidden_dim, self.num_labels)\n",
    "        \n",
    "        self.NEG_INF = -1.0e6\n",
    "        \n",
    "        if not args.use_bert:\n",
    "            self.vocab_size, self.embedding_dim = embeddings.shape\n",
    "            self.embed_layer = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "            self.embed_layer.weight.data = torch.from_numpy(embeddings)\n",
    "            self.embed_layer.weight.requires_grad = True #TODO try false? \n",
    "\n",
    "        self.use_bert = args.use_bert\n",
    "\n",
    "    def forward(self, x, z, mask):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            word_embeddings -- torch Variable in shape of (batch_size, length, embed_dim)\n",
    "            z -- rationale (batch_size, length)\n",
    "            mask -- torch Variable in shape of (batch_size, length)\n",
    "        Outputs:\n",
    "            predict -- (batch_size, num_label)\n",
    "        \"\"\"        \n",
    "        if not self.use_bert:\n",
    "            word_embeddings = self.embed_layer(x) #(batch_size, length, embedding_dim)\n",
    "        else:\n",
    "            word_embeddings = embedding_func(x)\n",
    "        \n",
    "        masked_input = word_embeddings * z.unsqueeze(-1)\n",
    "        hiddens = self.encoder(masked_input, mask)\n",
    "        \n",
    "        max_hidden = torch.max(hiddens + (1 - mask * z).unsqueeze(1) * self.NEG_INF, dim=2)[0]\n",
    "        \n",
    "        predict = self.predictor(max_hidden)\n",
    "        return predict\n",
    "    \n",
    "    def test(self, df_test):\n",
    "        self.eval()\n",
    "        batch_size = 100\n",
    "        accuracy = 0\n",
    "        for i in range(len(df_test)//batch_size):\n",
    "            batch = df_test.iloc[i*batch_size:(i+1)*batch_size]\n",
    "            batch_x_, batch_m_, batch_y_ = generate_data(batch)\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            z = torch.ones_like(batch_x_).type(torch.cuda.FloatTensor)\n",
    "            _, predict =  torch.max(self.forward(batch_x_, z, batch_m_), dim=1)\n",
    "            accuracy += (predict == batch_y_).sum().item()\n",
    "\n",
    "        return accuracy / len(df_test)\n",
    "    \n",
    "    def fit(self, df_train, df_test):\n",
    "        train_acc = []\n",
    "        test_acc = []\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "#         optimizer = optim.SGD(self.parameters(), lr=0.001, momentum=0.9) #experiment with learning rate\n",
    "        optimizer = optim.Adam(filter(lambda x: x.requires_grad, self.parameters()), lr=0.001)\n",
    "\n",
    "        for i in range(5000):\n",
    "            self.train()\n",
    "            batch = df_train.sample(40, replace=True)\n",
    "            batch_x_, batch_m_, batch_y_ = generate_data(batch)\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "\n",
    "            z = torch.ones_like(batch_x_).type(torch.cuda.FloatTensor)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = self.forward(batch_x_, z, batch_m_)\n",
    "            loss = criterion(outputs, batch_y_)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            if i % 10 == 0:\n",
    "                print(\"\")\n",
    "                print(\"Iteration \", i)\n",
    "                test_accuracy = self.test(df_test)\n",
    "#                 train_accuracy = self.test(df_train)\n",
    "                \n",
    "                print(\"Test accuracy: \", test_accuracy)\n",
    "#                 print(\"Train accuracy: \", train_accuracy)\n",
    "                test_acc.append(test_accuracy)\n",
    "#                 train_acc.append(train_accuracy)\n",
    "\n",
    "        print('Finished Training')\n",
    "        return test_acc, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\v-kedere\\AppData\\Local\\Continuum\\anaconda3\\envs\\rnp_env\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-28a30845f95c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# cls.cuda()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mtest_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-40-72943f7e93dd>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, df_train, df_test)\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Iteration \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                 \u001b[0mtest_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m                 \u001b[0mtrain_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Test accuracy: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-72943f7e93dd>\u001b[0m in \u001b[0;36mtest\u001b[1;34m(self, df_test)\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;31m# get the inputs; data is a list of [inputs, labels]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_x_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_x_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_m_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m             \u001b[0maccuracy\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mbatch_y_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-72943f7e93dd>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, z, mask)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[0mmasked_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_embeddings\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[0mhiddens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmasked_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mmax_hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhiddens\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNEG_INF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\rnp_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-72943f7e93dd>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, embeddings, mask)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[0mseq_lengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m             \u001b[0mseq_lengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0minputs_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Argument():\n",
    "    def __init__(self):\n",
    "        self.cell_type = 'GRU'\n",
    "        self.embedding_dim = 768\n",
    "        self.num_labels = 2\n",
    "        self.hidden_dim = 400\n",
    "        self.layer_num = 1\n",
    "        self.cuda = True\n",
    "        self.use_bert = True\n",
    "\n",
    "# args = Argument()\n",
    "# cls = ClassifierModule(None, args)\n",
    "# cls.cuda()\n",
    "\n",
    "test_acc, train_acc = cls.fit(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\v-kedere\\AppData\\Local\\Continuum\\anaconda3\\envs\\rnp_env\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8528281164195497"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test(df_test):\n",
    "    cls.eval()\n",
    "    batch_size = 100\n",
    "    accuracy = 0\n",
    "    for i in range(len(df_test)//batch_size):\n",
    "        batch = df_test.iloc[i*batch_size:(i+1)*batch_size]\n",
    "        batch_x_, batch_m_, batch_y_ = generate_data(batch)\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        z = torch.ones_like(batch_x_).type(torch.cuda.FloatTensor)\n",
    "        _, predict =  torch.max(cls(batch_x_, z, batch_m_), dim=1)\n",
    "        accuracy += (predict == batch_y_).sum().item()\n",
    "\n",
    "    return accuracy / len(df_test)\n",
    "\n",
    "test(df_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
