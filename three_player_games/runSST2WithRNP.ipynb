{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from rationale_3players_sentence_classification_models import ClassifierModule, HardRationale3PlayerClassificationModel\n",
    "from rationale_3players_for_emnlp import HardRationale3PlayerClassificationModelForEmnlp\n",
    "\n",
    "import torch\n",
    "from transformers import *\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify arguments for the model and data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "DATA_FOLDER = os.path.join(\"../../sentiment_dataset/data/\")\n",
    "LABEL_COL = \"label\"\n",
    "TEXT_COL = \"sentence\"\n",
    "TO_LOWER = True\n",
    "MAX_LEN = 150\n",
    "BATCH_SIZE_PRED = 512\n",
    "TRAIN_SIZE = 0.6\n",
    "batch_size = 150\n",
    "TOKEN_CUTOFF = 75 - 18\n",
    "\n",
    "class Argument():\n",
    "    def __init__(self):\n",
    "        self.model_type = 'RNN'\n",
    "        self.cell_type = 'GRU'\n",
    "        self.hidden_dim = 400\n",
    "        self.embedding_dim = 768\n",
    "        self.kernel_size = 5\n",
    "        self.layer_num = 1\n",
    "        self.fine_tuning = False\n",
    "        self.z_dim = 2\n",
    "        self.gumbel_temprature = 0.1\n",
    "        self.cuda = True\n",
    "        self.batch_size = 150\n",
    "        self.mlp_hidden_dim = 50\n",
    "        self.dropout_rate = 0.4\n",
    "        self.use_relative_pos = True\n",
    "        self.max_pos_num = 20\n",
    "        self.pos_embedding_dim = -1\n",
    "        self.fixed_classifier = True\n",
    "        self.fixed_E_anti = True\n",
    "        self.lambda_sparsity = 1.0\n",
    "        self.lambda_continuity = 1.0\n",
    "        self.lambda_anti = 1.0\n",
    "        self.lambda_pos_reward = 0.1\n",
    "        self.exploration_rate = 0.05\n",
    "        self.highlight_percentage = 0.3\n",
    "        self.highlight_count = 8\n",
    "        self.count_tokens = 8\n",
    "        self.count_pieces = 4\n",
    "        self.lambda_acc_gap = 1.2\n",
    "        self.label_embedding_dim = 400\n",
    "        self.game_mode = '3player'\n",
    "        self.margin = 0.2\n",
    "#         self.lm_setting = 'single'\n",
    "        self.lm_setting = 'multiple'\n",
    "#         self.lambda_lm = 100.0\n",
    "        self.lambda_lm = 1.0\n",
    "        self.ngram = 4\n",
    "        self.with_lm = False\n",
    "        self.batch_size_ngram_eval = 5\n",
    "        self.lr=0.001\n",
    "        self.working_dir = '/dccstor/yum-dbqa/Rationale/structured_rationale/game_model_with_lm/beer_single_working_dir'\n",
    "        self.model_prefix = 'tmp.%s.highlight%.2f.cont%.2f'%(self.game_mode, \n",
    "                                                                             self.highlight_percentage, \n",
    "                                                                             self.lambda_continuity)\n",
    "        self.pre_trained_model_prefix = 'pre_trained_cls.model'\n",
    "\n",
    "        self.save_path = os.path.join(\"..\", \"models\")\n",
    "        self.model_prefix = \"sst2rnpmodel\"\n",
    "        self.save_best_model = True\n",
    "        self.num_labels = 2\n",
    "        \n",
    "args = Argument()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Layer\n",
    "\n",
    "We want to use the pre-trained BERT embeddings, which generates embedded word vectors from word tokens.\n",
    "\n",
    "#### Process for a single sentence\n",
    "1.) generate_tokens() takes the BERT tokenizer and a sentence and tokenizes this text, to a limit of TOKEN_CUTOFF tokens. If the number of tokens is less than TOKEN_CUTOFF, it pads the tokens with the BERT pad symbol. It also provides a mask that can be used to ignore any pad tokens in classifier models down the road.<br>\n",
    "2.) embedding_func() takes the tokens from generate_tokens and uses them to make a corresponding embedding.\n",
    "\n",
    "#### Multiple sentences\n",
    "get_all_tokens takes a pandas dataframe, and adds columns tokens and mask into that dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BertTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5683c12b5220>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpretrained_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"bert-base-uncased\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Load pre-trained model (weights)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bert-base-uncased'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BertTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "pretrained_weights = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "model.eval()\n",
    "\n",
    "def generate_tokens(tokenizer, text):\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    tokenized_text = tokenized_text[:TOKEN_CUTOFF - 2]\n",
    "    tokenized_text = [\"[CLS]\"] + tokenized_text + [\"[SEP]\"]\n",
    "    pad_length = TOKEN_CUTOFF - len(tokenized_text)\n",
    "    mask = [1] * len(tokenized_text) + [0] * pad_length\n",
    "    \n",
    "    tokenized_text = tokenized_text + [\"[PAD]\"] * pad_length\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    \n",
    "    return np.array(indexed_tokens), np.array(mask)\n",
    "    \n",
    "def embedding_func(tokens):\n",
    "    ones_mask = Variable(torch.from_numpy(np.ones((len(tokens), TOKEN_CUTOFF))))\n",
    "    if args.cuda:\n",
    "        ones_mask = ones_mask.cuda()\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(tokens, ones_mask)[0]\n",
    "    return embeddings\n",
    "\n",
    "def get_all_tokens(data):\n",
    "    l = []\n",
    "    m = []\n",
    "    for sentence in data:\n",
    "        token_list, mask = generate_tokens(tokenizer, sentence)\n",
    "        l.append(token_list)\n",
    "        m.append(mask)\n",
    "    tokens = pd.DataFrame({\"tokens\": l, \"mask\": m})\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = os.path.join(\"..\", \"datasets\", \"glove.6B.100d.txt\")\n",
    "CNT_THRESH = 10\n",
    "\n",
    "def generate_tokens_glove(word_vocab, text):\n",
    "    indexed_text = [word_vocab[word] if (counts[word] > CNT_THRESH) else word_vocab[\"<UNK>\"] for word in text.split()]\n",
    "    pad_length = TOKEN_CUTOFF - len(indexed_text)\n",
    "    mask = [1] * len(indexed_text) + [0] * pad_length\n",
    "    \n",
    "    indexed_text = indexed_text + [word_vocab[\"<PAD>\"]] * pad_length\n",
    "    \n",
    "    return np.array(indexed_text), np.array(mask)\n",
    "\n",
    "def get_all_tokens_glove(data):\n",
    "    l = []\n",
    "    m = []\n",
    "    for sentence in data:\n",
    "        token_list, mask = generate_tokens_glove(word_vocab, sentence)\n",
    "        l.append(token_list)\n",
    "        m.append(mask)\n",
    "    tokens = pd.DataFrame({\"tokens\": l, \"mask\": m})\n",
    "    return tokens\n",
    "\n",
    "def build_vocab(df):\n",
    "    d = {\"<PAD>\":0, \"<UNK>\":1}\n",
    "    counts = {}\n",
    "    for i in range(len(df)):\n",
    "        sentence = df.iloc[i][TEXT_COL]\n",
    "        for word in sentence.split():\n",
    "            if word not in d:\n",
    "                d[word] = len(d)\n",
    "                counts[word] = 1\n",
    "            else:\n",
    "                counts[word] += 1\n",
    "    return d, counts\n",
    "\n",
    "def initial_embedding(word_vocab, embedding_size, embedding_path=None): \n",
    "    vocab_size = len(word_vocab)\n",
    "    # initialize a numpy embedding matrix \n",
    "    \n",
    "    embeddings = 0.1*np.random.randn(vocab_size, embedding_size).astype(np.float32)\n",
    "    \n",
    "    # replace the <PAD> embedding by all zero\n",
    "    embeddings[0, :] = np.zeros(embedding_size, dtype=np.float32)\n",
    "\n",
    "    if embedding_path and os.path.isfile(embedding_path):\n",
    "        f = open(embedding_path, \"r\", encoding=\"utf8\")\n",
    "        counter = 0\n",
    "        for line in f:\n",
    "            data = line.strip().split(\" \")\n",
    "            word = data[0].strip()\n",
    "            embedding = data[1::]\n",
    "            embedding = list(map(np.float32, embedding))\n",
    "            if word in word_vocab:\n",
    "                embeddings[word_vocab[word], :] = embedding\n",
    "                counter += 1\n",
    "        f.close()\n",
    "        print(\"%d words has been switched.\"%counter)\n",
    "    else:\n",
    "        print(\"embedding is initialized fully randomly.\")\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def load_data(fpath):\n",
    "    df_dict = {LABEL_COL: [], TEXT_COL: []}\n",
    "    with open(fpath, 'r') as f:\n",
    "        label_start = 0\n",
    "        sentence_start = 2\n",
    "        for line in f:\n",
    "            label = int(line[label_start])\n",
    "            sentence = line[sentence_start:]\n",
    "            df_dict[LABEL_COL].append(label)\n",
    "            df_dict[TEXT_COL].append(sentence)\n",
    "    return pd.DataFrame.from_dict(df_dict)\n",
    "\n",
    "\n",
    "df_train = load_data(os.path.join(DATA_FOLDER, 'stsa.binary.test'))\n",
    "\n",
    "# word_vocab, counts = build_vocab(df_test)\n",
    "# embeddings = initial_embedding(word_vocab, 100, glove_path)\n",
    "\n",
    "\n",
    "# df_test = load_data(os.path.join(DATA_FOLDER, 'stsa.binary.test'))\n",
    "\n",
    "# create training and testing labels\n",
    "y_train = df_train[LABEL_COL]\n",
    "# y_test = df_test[LABEL_COL]\n",
    "\n",
    "# create training and testing inputs\n",
    "X_train = df_train[TEXT_COL]\n",
    "# X_test = df_test[TEXT_COL]\n",
    "\n",
    "df_train = pd.concat([df_train, get_all_tokens_glove(X_train)], axis=1)\n",
    "# df_test = pd.concat([df_test, get_all_tokens(X_test)], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "      <th>tokens</th>\n",
       "      <th>mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>no movement , no yuks , not much of anything .\\n</td>\n",
       "      <td>[2, 1, 4, 2, 1, 4, 6, 7, 8, 9, 10, 0, 0, 0, 0,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>a gob of drivel so sickly sweet , even the eag...</td>\n",
       "      <td>[11, 1, 8, 1, 14, 1, 1, 4, 17, 18, 1, 1, 8, 1,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>gangs of new york is an unapologetic mess , wh...</td>\n",
       "      <td>[1, 8, 34, 1, 36, 37, 1, 39, 4, 40, 41, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>we never really feel involved with the story ,...</td>\n",
       "      <td>[51, 52, 53, 54, 1, 56, 18, 57, 4, 58, 59, 8, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>this is one of polanski 's best films .\\n</td>\n",
       "      <td>[65, 36, 66, 8, 1, 22, 68, 69, 10, 0, 0, 0, 0,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>0</td>\n",
       "      <td>an often-deadly boring , strange reading of a ...</td>\n",
       "      <td>[37, 1, 790, 4, 1, 1, 8, 11, 1, 40, 1, 2532, 3...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>0</td>\n",
       "      <td>the problem with concept films is that if the ...</td>\n",
       "      <td>[18, 1, 56, 1, 69, 36, 44, 120, 18, 1, 36, 11,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>0</td>\n",
       "      <td>safe conduct , however ambitious and well-inte...</td>\n",
       "      <td>[1, 1, 4, 1, 1, 104, 1, 4, 1, 111, 1, 18, 1461...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1819</th>\n",
       "      <td>0</td>\n",
       "      <td>a film made with as little wit , interest , an...</td>\n",
       "      <td>[11, 163, 1166, 56, 58, 109, 1, 4, 1, 4, 104, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820</th>\n",
       "      <td>0</td>\n",
       "      <td>but here 's the real damn : it is n't funny , ...</td>\n",
       "      <td>[89, 835, 22, 18, 838, 1, 63, 27, 36, 222, 684...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1821 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                           sentence  \\\n",
       "0         0   no movement , no yuks , not much of anything .\\n   \n",
       "1         0  a gob of drivel so sickly sweet , even the eag...   \n",
       "2         0  gangs of new york is an unapologetic mess , wh...   \n",
       "3         0  we never really feel involved with the story ,...   \n",
       "4         1          this is one of polanski 's best films .\\n   \n",
       "...     ...                                                ...   \n",
       "1816      0  an often-deadly boring , strange reading of a ...   \n",
       "1817      0  the problem with concept films is that if the ...   \n",
       "1818      0  safe conduct , however ambitious and well-inte...   \n",
       "1819      0  a film made with as little wit , interest , an...   \n",
       "1820      0  but here 's the real damn : it is n't funny , ...   \n",
       "\n",
       "                                                 tokens  \\\n",
       "0     [2, 1, 4, 2, 1, 4, 6, 7, 8, 9, 10, 0, 0, 0, 0,...   \n",
       "1     [11, 1, 8, 1, 14, 1, 1, 4, 17, 18, 1, 1, 8, 1,...   \n",
       "2     [1, 8, 34, 1, 36, 37, 1, 39, 4, 40, 41, 1, 1, ...   \n",
       "3     [51, 52, 53, 54, 1, 56, 18, 57, 4, 58, 59, 8, ...   \n",
       "4     [65, 36, 66, 8, 1, 22, 68, 69, 10, 0, 0, 0, 0,...   \n",
       "...                                                 ...   \n",
       "1816  [37, 1, 790, 4, 1, 1, 8, 11, 1, 40, 1, 2532, 3...   \n",
       "1817  [18, 1, 56, 1, 69, 36, 44, 120, 18, 1, 36, 11,...   \n",
       "1818  [1, 1, 4, 1, 1, 104, 1, 4, 1, 111, 1, 18, 1461...   \n",
       "1819  [11, 163, 1166, 56, 58, 109, 1, 4, 1, 4, 104, ...   \n",
       "1820  [89, 835, 22, 18, 838, 1, 63, 27, 36, 222, 684...   \n",
       "\n",
       "                                                   mask  \n",
       "0     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...  \n",
       "1     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4     [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                 ...  \n",
       "1816  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1817  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1818  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1819  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1820  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...  \n",
       "\n",
       "[1821 rows x 4 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "We read the data from files that already have it split between test and train sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fpath):\n",
    "    df_dict = {LABEL_COL: [], TEXT_COL: []}\n",
    "    with open(fpath, 'r') as f:\n",
    "        label_start = 0\n",
    "        sentence_start = 2\n",
    "        for line in f:\n",
    "            label = int(line[label_start])\n",
    "            sentence = line[sentence_start:]\n",
    "            df_dict[LABEL_COL].append(label)\n",
    "            df_dict[TEXT_COL].append(sentence)\n",
    "    return pd.DataFrame.from_dict(df_dict)\n",
    "\n",
    "df_train = load_data(os.path.join(DATA_FOLDER, 'stsa.binary.test'))\n",
    "df_test = load_data(os.path.join(DATA_FOLDER, 'stsa.binary.test'))\n",
    "\n",
    "# create training and testing labels\n",
    "y_train = df_train[LABEL_COL]\n",
    "y_test = df_test[LABEL_COL]\n",
    "\n",
    "# create training and testing inputs\n",
    "X_train = df_train[TEXT_COL]\n",
    "X_test = df_test[TEXT_COL]\n",
    "\n",
    "df_train = pd.concat([df_train, get_all_tokens(X_train)], axis=1)\n",
    "df_test = pd.concat([df_test, get_all_tokens(X_test)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, ...\n",
       "1       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
       "2       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
       "3       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
       "4       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...\n",
       "                              ...                        \n",
       "1816    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
       "1817    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
       "1818    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
       "1819    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
       "1820    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...\n",
       "Name: mask, Length: 1821, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\v-kedere\\AppData\\Local\\Continuum\\anaconda3\\envs\\rnp_env\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "args = Argument()\n",
    "\n",
    "classification_model = HardRationale3PlayerClassificationModelForEmnlp(embedding_func, args)\n",
    "\n",
    "if args.cuda:\n",
    "    classification_model.cuda()\n",
    "\n",
    "classification_model.init_optimizers()\n",
    "classification_model.init_C_model()\n",
    "\n",
    "args.fixed_E_anti = False\n",
    "classification_model.fixed_E_anti = args.fixed_E_anti\n",
    "args.with_lm = False\n",
    "args.lambda_lm = 1.0\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "dev_accs = [0.0]\n",
    "dev_anti_accs = [0.0]\n",
    "dev_cls_accs = [0.0]\n",
    "test_accs = [0.0]\n",
    "test_anti_accs = [0.0]\n",
    "test_cls_accs = [0.0]\n",
    "best_dev_acc = 0.0\n",
    "best_test_acc = 0.0\n",
    "num_iteration = 100\n",
    "display_iteration = 1\n",
    "test_iteration = 1\n",
    "\n",
    "eval_accs = [0.0]\n",
    "eval_anti_accs = [0.0]\n",
    "\n",
    "queue_length = 200\n",
    "z_history_rewards = deque(maxlen=queue_length)\n",
    "z_history_rewards.append(0.)\n",
    "\n",
    "classification_model.init_optimizers()\n",
    "classification_model.init_rl_optimizers()\n",
    "classification_model.init_reward_queue()\n",
    "\n",
    "old_E_anti_weights = classification_model.E_anti_model.predictor._parameters['weight'][0].cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_data(batch):\n",
    "    x_mat = np.stack(batch[\"tokens\"], axis=0)\n",
    "    x_mask = np.stack(batch[\"mask\"], axis=0)\n",
    "    y_vec = np.stack(batch[\"label\"], axis=0)\n",
    "    \n",
    "    batch_x_ = Variable(torch.from_numpy(x_mat)).to(torch.int64)\n",
    "    batch_m_ = Variable(torch.from_numpy(x_mask)).type(torch.FloatTensor)\n",
    "    batch_y_ = Variable(torch.from_numpy(y_vec)).to(torch.int64)\n",
    "\n",
    "    if args.cuda:\n",
    "        batch_x_ = batch_x_.cuda()\n",
    "        batch_m_ = batch_m_.cuda()\n",
    "        batch_y_ = batch_y_.cuda()\n",
    "\n",
    "    return batch_x_, batch_m_, batch_y_\n",
    "\n",
    "def _get_sparsity(z, mask):\n",
    "    mask_z = z * mask\n",
    "    seq_lengths = torch.sum(mask, dim=1)\n",
    "\n",
    "    sparsity_ratio = torch.sum(mask_z, dim=-1) / seq_lengths #(batch_size,)\n",
    "#     sparsity_count = torch.sum(mask_z, dim=-1)\n",
    "\n",
    "    return sparsity_ratio\n",
    "\n",
    "def _get_continuity(z, mask):\n",
    "    mask_z = z * mask\n",
    "    seq_lengths = torch.sum(mask, dim=1)\n",
    "    \n",
    "    mask_z_ = torch.cat([mask_z[:, 1:], mask_z[:, -1:]], dim=-1)\n",
    "        \n",
    "    continuity_ratio = torch.sum(torch.abs(mask_z - mask_z_), dim=-1) / seq_lengths #(batch_size,) \n",
    "#     continuity_count = torch.sum(torch.abs(mask_z - mask_z_), dim=-1)\n",
    "    \n",
    "    return continuity_ratio\n",
    "\n",
    "def display_example(x, m, z):\n",
    "    seq_len = int(m.sum().item())\n",
    "    ids = x[:seq_len]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "    \n",
    "    final = \"\"\n",
    "    for i in range(1, len(tokens) - 1):\n",
    "        if z[i]:\n",
    "            final += \"[\" + tokens[i] + \"]\"\n",
    "        else:\n",
    "            final += tokens[i]\n",
    "        final += \" \"\n",
    "    print(final)\n",
    "\n",
    "def test():\n",
    "    classification_model.eval()\n",
    "    \n",
    "    test_size = 200\n",
    "    \n",
    "    test_batch = df_test.sample(test_size)\n",
    "    batch_x_, batch_m_, batch_y_ = generate_data(test_batch)\n",
    "    predict, anti_predict, z, neg_log_probs = classification_model(batch_x_, batch_m_)\n",
    "    \n",
    "    # do a softmax on the predicted class probabilities\n",
    "    _, y_pred = torch.max(predict, dim=1)\n",
    "    _, anti_y_pred = torch.max(anti_predict, dim=1)\n",
    "    \n",
    "    # calculate sparsity\n",
    "    print(\"Test sparsity: \", _get_sparsity(z, batch_m_).sum().item() / batch_size)\n",
    "    \n",
    "    accuracy = (y_pred == batch_y_).sum().item() / test_size\n",
    "    anti_accuracy = (anti_y_pred == batch_y_).sum().item() / test_size\n",
    "    train_accs.append((accuracy, anti_accuracy))\n",
    "    print(\"Test accuracy: \", accuracy, \"% Anti-accuracy: \", anti_accuracy)\n",
    "\n",
    "    # display an example\n",
    "    print(\"Gold Label: \", batch_y_[0].item(), \" Pred label: \", y_pred[0].item())\n",
    "    display_example(batch_x_[0], batch_m_[0], z[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sparsity:  0.3383092498779297\n",
      "Test accuracy:  0.45 % Anti-accuracy:  0.525\n",
      "Gold Label:  0  Pred label:  0\n",
      "it does n ' t do the original [any] particular dish ##ono ##r , but neither does it ex [##ude] [any] [charm] [or] [personality] . \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (48) must match the size of tensor b (57) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-2761e838af95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mbatch_x_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassification_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_cls_one_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_x_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_m_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# calculate classification accuarcy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\maidap\\three_player_for_emnlp\\three_player_games\\rationale_3players_for_emnlp.py\u001b[0m in \u001b[0;36mtrain_cls_one_step\u001b[1;34m(self, x, label, mask)\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain_cls_one_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_cls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[0me_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\maidap\\three_player_for_emnlp\\three_player_games\\rationale_3players_for_emnlp.py\u001b[0m in \u001b[0;36mforward_cls\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\rnp_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\maidap\\three_player_for_emnlp\\three_player_games\\rationale_3players_sentence_classification_models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, word_embeddings, z, mask)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[0mhiddens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmasked_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0mmax_hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhiddens\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNEG_INF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_hidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (48) must match the size of tensor b (57) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "test_freq = 50\n",
    "save_freq = 999\n",
    "\n",
    "for iteration in range(10000):\n",
    "    classification_model.train()\n",
    "\n",
    "    # sample a batch of data\n",
    "    batch = df_train.sample(batch_size, replace=True)\n",
    "    batch_x_, batch_m_, batch_y_ = generate_data(batch)\n",
    "\n",
    "    losses, predict = classification_model.train_cls_one_step(batch_x_, batch_y_, batch_m_)\n",
    "\n",
    "    # calculate classification accuarcy\n",
    "    _, y_pred = torch.max(predict, dim=1)\n",
    "\n",
    "    acc = np.float((y_pred == batch_y_).sum().cpu().data.item()) / args.batch_size\n",
    "    train_accs.append(acc)\n",
    "    \n",
    "    if iteration % save_freq == 0:\n",
    "        torch.save(classification_model.state_dict(), \"iteration_\" + str(iteration) + \".pth\")\n",
    "    if iteration % test_freq == 0:\n",
    "        test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.iloc[0]['mask']\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierModule(\n",
       "  (encoder): RnnModel(\n",
       "    (rnn_layer): GRU(768, 200, bidirectional=True)\n",
       "  )\n",
       "  (predictor): Linear(in_features=400, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RnnModel(nn.Module):\n",
    "\n",
    "    def __init__(self, args, input_dim):\n",
    "        \"\"\"\n",
    "        args.hidden_dim -- dimension of filters\n",
    "        args.embedding_dim -- dimension of word embeddings\n",
    "        args.layer_num -- number of RNN layers   \n",
    "        args.cell_type -- type of RNN cells, GRU or LSTM\n",
    "        \"\"\"\n",
    "        super(RnnModel, self).__init__()\n",
    "        \n",
    "        self.args = args\n",
    " \n",
    "        if args.cell_type == 'GRU':\n",
    "            self.rnn_layer = nn.GRU(input_size=input_dim, \n",
    "                                    hidden_size=args.hidden_dim//2, \n",
    "                                    num_layers=args.layer_num, bidirectional=True)\n",
    "        elif args.cell_type == 'LSTM':\n",
    "            self.rnn_layer = nn.LSTM(input_size=input_dim, \n",
    "                                     hidden_size=args.hidden_dim//2, \n",
    "                                     num_layers=args.layer_num, bidirectional=True)\n",
    "    \n",
    "    def forward(self, embeddings, mask=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embeddings -- sequence of word embeddings, (batch_size, sequence_length, embedding_dim)\n",
    "            mask -- a float tensor of masks, (batch_size, length)\n",
    "        Outputs:\n",
    "            hiddens -- sentence embedding tensor, (batch_size, hidden_dim, sequence_length)\n",
    "        \"\"\"\n",
    "        embeddings_ = embeddings.transpose(0, 1) #(sequence_length, batch_size, embedding_dim)\n",
    "        \n",
    "        if mask is not None: #TODO change\n",
    "            seq_lengths = list(torch.sum(mask, dim=1).cpu().data.numpy())\n",
    "            seq_lengths = list(map(int, seq_lengths))\n",
    "            inputs_ = torch.nn.utils.rnn.pack_padded_sequence(embeddings_, seq_lengths, enforce_sorted=False)\n",
    "        else:\n",
    "            inputs_ = embeddings_\n",
    "        \n",
    "        hidden, _ = self.rnn_layer(inputs_) #(sequence_length, batch_size, hidden_dim (* 2 if bidirectional))\n",
    "        \n",
    "        if mask is not None: #TODO change\n",
    "            hidden, _ = torch.nn.utils.rnn.pad_packed_sequence(hidden) #(length, batch_size, hidden_dim)\n",
    "        \n",
    "        return hidden.permute(1, 2, 0) #(batch_size, hidden_dim, sequence_length)\n",
    "\n",
    "class ClassifierModule(nn.Module):\n",
    "    '''\n",
    "    classifier for both E and E_anti models\n",
    "    '''\n",
    "    def __init__(self, args):\n",
    "        super(ClassifierModule, self).__init__()\n",
    "        self.args = args\n",
    "        \n",
    "        self.num_labels = args.num_labels\n",
    "        self.hidden_dim = args.hidden_dim\n",
    "        self.mlp_hidden_dim = args.mlp_hidden_dim #50\n",
    "        \n",
    "        self.input_dim = args.embedding_dim\n",
    "        \n",
    "        self.encoder = RnnModel(self.args, self.input_dim)\n",
    "        self.predictor = nn.Linear(self.hidden_dim, self.num_labels)\n",
    "        \n",
    "        self.NEG_INF = -1.0e6\n",
    "        \n",
    "\n",
    "    def forward(self, word_embeddings, z, mask):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            word_embeddings -- torch Variable in shape of (batch_size, length, embed_dim)\n",
    "            z -- rationale (batch_size, length)\n",
    "            mask -- torch Variable in shape of (batch_size, length)\n",
    "        Outputs:\n",
    "            predict -- (batch_size, num_label)\n",
    "        \"\"\"        \n",
    "\n",
    "        masked_input = word_embeddings * z.unsqueeze(-1)\n",
    "        hiddens = self.encoder(masked_input, mask)\n",
    "        \n",
    "        max_hidden = torch.max(hiddens + (1 - mask * z).unsqueeze(1) * self.NEG_INF, dim=2)[0]\n",
    "        \n",
    "        predict = self.predictor(max_hidden)\n",
    "\n",
    "        return predict\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, args, input_dim):\n",
    "        \"\"\"        \n",
    "        args.z_dim -- rationale or not, always 2\n",
    "        args.model_type -- \"CNN\" or \"RNN\"\n",
    "\n",
    "        if CNN:\n",
    "            args.hidden_dim -- dimension of filters\n",
    "            args.embedding_dim -- dimension of word embeddings\n",
    "            args.kernel_size -- kernel size of the conv1d\n",
    "            args.layer_num -- number of CNN layers        \n",
    "        if use RNN:\n",
    "            args.hidden_dim -- dimension of filters\n",
    "            args.embedding_dim -- dimension of word embeddings\n",
    "            args.layer_num -- number of RNN layers   \n",
    "            args.cell_type -- type of RNN cells, \"GRU\" or \"LSTM\"\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.args = args\n",
    "        self.z_dim = args.z_dim\n",
    "        \n",
    "        if args.model_type == \"CNN\":\n",
    "            self.generator_model = CnnModel(args, input_dim)\n",
    "        elif args.model_type == \"RNN\":\n",
    "            self.generator_model = RnnModel(args, input_dim)\n",
    "        self.output_layer = nn.Linear(args.hidden_dim, self.z_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Given input x in shape of (batch_size, sequence_length) generate a \n",
    "        \"binary\" mask as the rationale\n",
    "        Inputs:\n",
    "            x -- input sequence of word embeddings, (batch_size, sequence_length, embedding_dim)\n",
    "        Outputs:\n",
    "            z -- output rationale, \"binary\" mask, (batch_size, sequence_length)\n",
    "        \"\"\"\n",
    "        \n",
    "        #(batch_size, sequence_length, hidden_dim)\n",
    "        hiddens = self.generator_model(x, mask).transpose(1, 2).contiguous() \n",
    "        scores = self.output_layer(hiddens) # (batch_size, sequence_length, 2)\n",
    "\n",
    "        return scores\n",
    "\n",
    "import torch.nn.functional as F\n",
    "    \n",
    "rnn = RnnModel(args, 768)\n",
    "rnn.cuda()\n",
    "\n",
    "gen = Generator(args, 768)\n",
    "gen.cuda()\n",
    "\n",
    "cls = ClassifierModule(args)\n",
    "cls.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = df_test.iloc[0:1]\n",
    "batch_x_, batch_m_, batch_y_ = generate_data(test_batch)\n",
    "embeds = embedding_func(batch_x_)\n",
    "embeds.cuda()\n",
    "\n",
    "z = Variable(torch.from_numpy(np.array([[ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1,  1]]))).to(torch.int64)\n",
    "z.cuda()\n",
    "\n",
    "hiddens = rnn(embeds, batch_m_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (14) must match the size of tensor b (57) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-c29803536898>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmax_hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhiddens\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbatch_m_\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1e-6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (14) must match the size of tensor b (57) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "max_hidden = torch.max(hiddens + (1 - batch_m_ * z.cuda()).unsqueeze(1).cuda() * 1e-6, dim=2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([[1.0000e-06]], device='cuda:0'),\n",
       "indices=tensor([[16]], device='cuda:0'))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max((1 - batch_m_ * z.cuda()).unsqueeze(1).cuda() * 1e-6, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected device cuda:0 but got device cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-135-f03771307464>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0membeds\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: expected device cuda:0 but got device cpu"
     ]
    }
   ],
   "source": [
    "embeds * z.unsqueeze(-1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_rationales(z_prob_):\n",
    "    '''\n",
    "    Input:\n",
    "        z_prob_ -- (num_rows, length, 2)\n",
    "    Output:\n",
    "        z -- (num_rows, length)\n",
    "    '''        \n",
    "    z_prob__ = z_prob_.view(-1, 2) # (num_rows * length, 2)\n",
    "\n",
    "    # sample actions\n",
    "    sampler = torch.distributions.Categorical(z_prob__)\n",
    "    if True:\n",
    "        z_ = sampler.sample() # (num_rows * p_length,)\n",
    "    else:\n",
    "        z_ = torch.max(z_prob__, dim=-1)[1]\n",
    "\n",
    "    #(num_rows, length)\n",
    "    z = z_.view(z_prob_.size(0), z_prob_.size(1))\n",
    "\n",
    "    if True == True:\n",
    "        z = z.type(torch.cuda.FloatTensor)\n",
    "    else:\n",
    "        z = z.type(torch.FloatTensor)\n",
    "\n",
    "    # (num_rows * length,)\n",
    "    neg_log_probs_ = -sampler.log_prob(z_)\n",
    "    # (num_rows, length)\n",
    "    neg_log_probs = neg_log_probs_.view(z_prob_.size(0), z_prob_.size(1))\n",
    "\n",
    "    return z, neg_log_probs\n",
    "\n",
    "z_scores_ = gen(embeds, batch_m_)\n",
    "z_scores_[:, :, 1] = z_scores_[:, :, 1] + (1 - batch_m_).cuda() * -1e6\n",
    "\n",
    "z_probs_ = F.softmax(z_scores_, dim=-1)\n",
    "\n",
    "z_probs_temp_ = (batch_m_.unsqueeze(-1).cuda() * ( ((1 - .005) * z_probs_ + .005).cuda() / z_probs_.size(-1) ) )\n",
    "z_probs_ = z_probs_ + ((1 - batch_m_.unsqueeze(-1)).cuda() * z_probs_temp_)\n",
    "\n",
    "z, neg_log_probs = _generate_rationales(z_probs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1.]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (57) must match the size of tensor b (14) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-160-48018604b4c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_m_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\rnp_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-153-84982d9678f5>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, word_embeddings, z, mask)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \"\"\"        \n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mmasked_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_embeddings\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[0mhiddens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmasked_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (57) must match the size of tensor b (14) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "cls(embeds, z, batch_m_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_m_ = Variable(torch.from_numpy(np.array([[ 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1,  1]]))).to(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 14]), torch.Size([1, 14]))"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape, batch_m_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 57, 768])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
